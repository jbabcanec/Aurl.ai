# Stage 2: Augmented Training
# Rich training with 5x data variety through augmentation

model:
  mode: "transformer"  # Continue with transformer, but richer data
  d_model: 384         # Increased capacity for augmented data
  n_layers: 6          # Deeper for more complex patterns
  n_heads: 6           # More attention heads
  dropout: 0.15        # Slightly higher dropout for regularization
  max_sequence_length: 1024  # Longer sequences for richer patterns
  vocab_size: 774
  
  # VAE specific (still not used but larger for stage 3)
  latent_dim: 192
  encoder_layers: 4
  decoder_layers: 4
  beta: 1.0
  
  # Enhanced attention for longer sequences
  attention_type: "hierarchical"
  local_window_size: 256
  global_window_size: 128
  sliding_window_size: 512

training:
  batch_size: 6        # Slightly smaller due to augmentation overhead
  learning_rate: 2.0e-4  # Lower LR for stability with augmentation
  num_epochs: 30       # More epochs to learn from augmented data
  warmup_steps: 1000   # Longer warmup for stability
  weight_decay: 1.0e-5
  gradient_clip_norm: 1.0
  accumulate_grad_batches: 2  # Accumulate gradients for effective larger batch
  
  # Checkpointing
  save_every_n_epochs: 5
  keep_best_n_checkpoints: 3
  
  # Early stopping
  early_stopping: true
  patience: 10         # More patience for augmented data
  min_delta: 5.0e-5    # Smaller delta for fine improvements
  
  # Scheduling
  scheduler: "cosine"
  min_learning_rate: 5.0e-7
  
  # Mixed precision
  mixed_precision: false
  
  # Loss weights
  reconstruction_weight: 1.0
  kl_weight: 1.0
  adversarial_weight: 0.0  # Still no GAN

data:
  sequence_length: 1024    # Longer sequences for richer patterns
  overlap: 256
  min_sequence_length: 128
  
  # Preprocessing
  normalize_velocity: true
  quantize_timing: true
  quantization_level: 1.0
  
  # FULL AUGMENTATION in Stage 2 - 5x data variety
  augmentation:
    transpose: true
    transpose_range: [-6, 6]      # Full range transposition
    time_stretch: true
    time_stretch_range: [0.85, 1.15]  # Moderate tempo variation
    velocity_scale: true
    velocity_scale_range: [0.7, 1.3]  # Dynamic range variation
    instrument_substitution: true
    substitution_probability: 0.3
    rhythmic_variation: true
    swing_amount: 0.1
    probability: 0.7         # 70% chance of augmentation per sample
  
  # Caching
  cache_processed_data: true
  cache_size_gb: 8         # More cache for augmented data
  num_workers: 6

experiment:
  name: "stage2_augmented_training"
  group: "aurl_training_pipeline"
  tags: ["stage2", "augmented", "5x_data", "transformer"]
  notes: "Stage 2: Rich training with full augmentation pipeline"
  
  # Tracking
  use_wandb: false
  use_tensorboard: true
  tensorboard_log_dir: "logs/tensorboard/stage2"
  
  # Logging
  log_every_n_steps: 50
  save_samples_every_n_epochs: 5

system:
  device: "auto"
  gpu_ids: [0]
  num_workers: 6
  pin_memory: true
  
  # Paths
  data_dir: "data/raw"
  cache_dir: "data/cache"
  output_dir: "outputs"
  log_dir: "logs"
  
  # Reproducibility
  seed: 123            # Different seed for variation
  deterministic: false  # Allow some randomness for augmentation
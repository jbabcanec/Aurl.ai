# High quality training configuration
# For producing best possible results

training:
  batch_size: 32
  learning_rate: 1.0e-4
  num_epochs: 500
  warmup_steps: 5000
  weight_decay: 1.0e-6
  
  # Conservative early stopping
  early_stopping: true
  patience: 50
  min_delta: 1.0e-6
  
  # Careful checkpointing
  save_every_n_epochs: 25
  keep_best_n_checkpoints: 10
  
  # Optimized scheduling
  scheduler: "cosine"
  min_learning_rate: 1.0e-7
  
  # Refined loss weights
  reconstruction_weight: 1.0
  kl_weight: 0.8
  adversarial_weight: 0.3

data:
  sequence_length: 4096
  overlap: 1024
  
  # Extensive augmentation
  augmentation:
    probability: 0.8
    transpose_range: [-12, 12]
    time_stretch_range: [0.7, 1.3]
    velocity_scale_range: [0.6, 1.4]

experiment:
  name: "high_quality_training"
  tags: ["high_quality", "production"]
  use_wandb: true
  log_every_n_steps: 200
  save_samples_every_n_epochs: 25